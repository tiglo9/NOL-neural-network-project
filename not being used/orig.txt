
# # Do the full training algorithm
# train_losses = []
# validation_losses = []
# train_accuracies = []
# validation_accuracies = []
# for epoch in range(1, epochs+1):
#     # (Re)set the training loss for this epoch.
#     train_loss = 0.0
#     correctly_classified = 0
#     for batch in tqdm(train_loader, desc=f"Training epoch {epoch}"):
#         # Reset the gradients so that we start fresh.
#         neural_network.reset_gradients()

#         # Also reset the adam weights
#         neural_network.reset_adam_params()

#         # Get the images and labels from the batch
#         images = np.vstack([image for (image, _) in batch])
#         labels = np.vstack([label for (_, label) in batch])

#         # Wrap images and labels in a Value class.
#         images = Value(images, expr="X")
#         labels = Value(labels, expr="Y")

#         # Compute what the model says is the label.
#         output = neural_network(images)

#         # Compute the loss for this batch.
#         loss = mse_loss(
#             output,
#             labels
#         )

#         # Do backpropagation
#         loss.backward()

#         # Update the weights and biases using the chosen algorithm, in this case gradient descent.
#         neural_network.adam_descent(learning_rate, 1e-10)

#         # Store the loss for this batch.
#         train_loss += loss.data

#         # Store accuracies for extra interpretability
#         true_classification = np.argmax(
#             labels.data,
#             axis=1
#         )
#         predicted_classification = np.argmax(
#             output.data,
#             axis=1
#         )
#         correctly_classified += np.sum(true_classification == predicted_classification)

#     # Store the loss and average accuracy for the entire epoch.
#     train_losses.append(train_loss)
#     train_accuracies.append(correctly_classified / train_dataset_size)

#     print(f"Accuracy: {train_accuracies[-1]}")
#     print(f"Loss: {train_loss}")
#     print("")

#     validation_loss = 0.0
#     correctly_classified = 0
#     for batch in tqdm(validation_loader, desc=f"Validation epoch {epoch}"):
#         # Get the images and labels from the batch
#         images = np.vstack([image for (image, _) in batch])
#         labels = np.vstack([label for (_, label) in batch])

#         # Wrap images and labels in a Value class.
#         images = Value(images, expr="X")
#         labels = Value(labels, expr="Y")

#         # Compute what the model says is the label.
#         output = neural_network(images)

#         # Compute the loss for this batch.
#         loss = mse_loss(
#             output,
#             labels
#         )

#         # Store the loss for this batch.
#         validation_loss += loss.data

#         # Store accuracies for extra interpretability
#         true_classification = np.argmax(
#             labels.data,
#             axis=1
#         )
#         predicted_classification = np.argmax(
#             output.data,
#             axis=1
#         )
#         correctly_classified += np.sum(true_classification == predicted_classification)

#     validation_losses.append(validation_loss)
#     validation_accuracies.append(correctly_classified / validation_dataset_size)

#     print(f"Accuracy: {validation_accuracies[-1]}")
#     print(f"Loss: {validation_loss}")
#     print("")

# print(" === SUMMARY === ")
# print(" --- training --- ")
# print(f"Accuracies: {train_accuracies}")
# print(f"Losses: {train_losses}")
# print("")
# print(" --- validation --- ")
# print(f"Accuracies: {validation_accuracies}")
# print(f"Losses: {validation_losses}")
# print("")
